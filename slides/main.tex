\documentclass[aspectratio=169]{beamer}
\beamertemplatenavigationsymbolsempty
\usecolortheme{beaver}
\setbeamertemplate{blocks}[rounded=true, shadow=true]
\setbeamertemplate{footline}[page number]


\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage[english,russian]{babel}
\usepackage{xcolor}



\usepackage[utf8]{inputenc}
\usepackage{amssymb,amsfonts,amsmath,mathtext}
%\usepackage{subfig}
\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{quotes}
\usepackage{tikzscale}
\usepackage{scalefnt}
\usepackage{xcolor}
\usepackage[all]{xy} % xy package for diagrams
\usepackage{array}
\usepackage{multicol}% many columns in slide
\usepackage{hyperref}% urls
\usepackage{hhline}%tables
\usepackage{booktabs}
% \usepackage{biblatex}



\def\bw{\mathbf{w}}
\def\balpha{\boldsymbol{\alpha}}
\def\ltrn{\mathcal{L}_{\mathrm{train}}}
\def\lval{\mathcal{L}_{\mathrm{val}}}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}


\definecolor{dark_green}{rgb}{0, 0.788, 0}
\definecolor{dark_red}{rgb}{0.9, 0, 0}


% Your figures are here:
\graphicspath{ {fig/} {../fig/} }

\definecolor{ao(english)}{rgb}{0.0, 0.5, 0.0}
\definecolor{bleudefrance}{rgb}{0.19, 0.55, 0.91}

%----------------------------------------------------------------------------------------------------------
% \title[\hbox to 56mm{Feature generation}]{Поиск согласованных нейросетевых моделей в задаче мультидоменного обучения}
\title[\hbox to 56mm{Feature generation}]{Обобщенная жадная градиентная оптимизация гиперпараметров}
% \author{К.\,Д.~Яковлев\inst{1} \and \and О.\,Ю.~Бахтеев\inst{1,2}\and В.\,В.~Стрижов\inst{1,2} \\
% \tt{\footnotesize \{iakovlev.kd, bakhteev, strijov\}@phystech.edu }}
% \institute{\inst{1} Москва, Московский физико-технический институт \and
% \inst{2} Москва, Вычислительный центр им. А.А. Дородницына ФИЦ ИУ РАН} \date{2023}
\author{К.\,Д.~Яковлев\inst{} \\
\tt{\footnotesize iakovlev.kd@phystech.edu }}
\institute{\inst{} Москва, Московский физико-технический институт \\
\textbf{Научный руководитель}: к.ф.-м.н. Бахтеев Олег Юрьевич} \date{2024}
%----------------------------------------------------------------------------------------------------------
\begin{document}
%----------------------------------------------------------------------------------------------------------
\begin{frame}
\thispagestyle{empty}
\maketitle
\end{frame}
%-----------------------------------------------------------------------------------------------------
\begin{frame}{Цель исследования}

\begin{block}{Цель} 
  Предложить градиентный метод оптимизации гиперпараметров с линейейной по количеству параметров и гиперпараметров
  сложностью итерации и затратами памяти.
\end{block}

~\\
\begin{block}{Проблема}
  Существующие методы не гарантируют выполнения следующих условий одновременно: 1) отсутствие требований на сходимость
  внутренней процедуры оптимизации к единственному решению,
  2) отсутствие смещения из-за короткого горизонта, 3) линейная сложность итерации и затраты памяти.
\end{block}
~\\
\begin{block}{Метод решения}
  Предлагаемый метод основан на агрегации жадных гиперградиентов без дополнительных вычислительных затрат.
\end{block}

\end{frame}


\begin{frame}{Агрегация жадных гиперградиентов}
  Пусть задано $\gamma \in (0, 1)$. Тогда аппроксимация гиперградиента запишется как:
  \begin{align*}
    \hat{d}_{\vect{\alpha}}\mathcal{L}_\text{val}(\vect{w}_T, \vect{\alpha}; \gamma) = \nabla_{\vect{\alpha}}\mathcal{L}_\text{val}(\vect{w}_T, \vect{\alpha}) + \sum_{t=1}^T\gamma^{T-t}\nabla_{\vect{w}_t}\mathcal{L}_\text{val}(\vect{w}_t, \vect{\alpha})\vect{B}_t.
  \end{align*}
  \begin{table}
    \begin{tabular}{c|c|c|c|c|c} \toprule
       & IFT & RMD & DrMAD & $T1-T2$ & Ours \\ \midrule
      Онлайн оптимизация & \color{dark_red}{\texttimes} & \color{dark_green}{\checkmark} & \color{dark_red}{\texttimes} & \color{dark_green}{\checkmark} & \color{dark_green}{\checkmark} \\
      Длинный горизонт & \color{dark_green}{\checkmark} & \color{dark_green}{\checkmark} & \color{dark_green}{\checkmark} & \color{dark_red}{\texttimes} & \color{dark_green}{\checkmark} \\
      Линейная сложность & \color{dark_green}{\checkmark} & \color{dark_red}{\texttimes} & \color{dark_green}{\checkmark} & \color{dark_green}{\checkmark} & \color{dark_green}{\checkmark} \\ \bottomrule
      
    \end{tabular}
  \end{table}
  
\end{frame}

%----------------------------------------------------------------------------------------------------------


\begin{frame}{Постановка задачи оптимизации гиперпараметров}
\begin{itemize}
  \item Пусть задан вектор параметров модели $\mathbf{w} \in \mathbb{R}^p$ и вектор гиперпараметров
  $\boldsymbol{\alpha} \in \mathbb{R}^h$. Задача оптимизации:
  \begin{align*}
    \boldsymbol{\alpha}^* &= \arg\min_{\boldsymbol{\alpha}}\mathcal{L}_2(\mathbf{w}^*, \boldsymbol{\alpha}), \\
    \mathrm{s.t.} \quad \mathbf{w}^* &= \arg\min_{\mathbf{w}}\mathcal{L}_1(\mathbf{w}, \boldsymbol{\alpha}).
  \end{align*}
  \item Пусть внутренняя задача решается с помощью оптимизатора $\mathbf{\Phi}(., .)$:
  \begin{align*}
    \mathbf{w}_{t + 1}(\boldsymbol{\alpha}) = \mathbf{\Phi}(\mathbf{w}_t, \boldsymbol{\alpha}), \quad
    t = \overline{1, T}; \quad \vect{\Phi}(\vect{w}_{t}, \vect{\alpha}) = \vect{w}_t - \eta\nabla_{\vect{w}_t}\ltrn(\vect{w}_t, \vect{\alpha})
  \end{align*}
  \item Гиперградиент запишется как:
  \begin{align*}
    &d_{\vect{\alpha}}\mathcal{L}_\text{val}(\vect{w}_T, \vect{\alpha}) = \nabla_{\vect{\alpha}}\mathcal{L}_\text{val}(\vect{w}_T, \vect{\alpha}) + \sum_{t=1}^T \nabla_{\vect{w}_T}\mathcal{L}_\text{val}(\vect{w}_T, \vect{\alpha})\left(\prod_{k=t+1}^T\vect{A}_k\right)\vect{B}_t, \\
    &\vect{A}_k = \frac{\partial\vect{\Phi}(\vect{w}_{k-1}, \vect{\alpha})}{\partial\vect{w}_{k-1}}, \quad \vect{B}_t = \frac{\partial\vect{\Phi}(\vect{w}_{t-1}, \vect{\alpha})}{\partial\vect{\alpha}}.
\end{align*}
\end{itemize}
\end{frame}


\begin{frame}{Аппроксимация гиперградиента}
  Пусть задано ${\color{olive}\gamma} \in (0, 1)$. Тогда аппроксимация гиперградиента запишется как:
  \begin{align*}
    {\color{violet}\hat{d}_{\vect{\alpha}}\mathcal{L}_\text{val}(\vect{w}_T, \vect{\alpha}; \gamma)} = \nabla_{\vect{\alpha}}\mathcal{L}_\text{val}(\vect{w}_T, \vect{\alpha}) + \sum_{t=1}^T{\color{olive}\gamma^{T-t}}{\color{teal}\nabla_{\vect{w}_t}\mathcal{L}_\text{val}(\vect{w}_t, \vect{\alpha})\vect{B}_t}.
  \end{align*}
  \begin{figure}
  \scalebox{0.9}{
  \begin{tikzpicture}
    %detailed
    \node at (0,0) [rectangle,draw] (gtm1) {$\nabla_{\vect{w}_{t-1}}\ltrn(\vect{w}_{t-1}, \vect{\alpha})$};
    \node at (0, -2) [circle,draw] (wtm1) {$\vect{w}_{t-1}$};
    \draw [-{Stealth[scale=2]}] (wtm1) to (gtm1);
    \node at (3,0) [circle,draw,minimum size = 0pt, inner sep=0pt,label=90:$-\eta$] (m1) {$\times$};
    \draw [-{Stealth[scale=2]}] (gtm1) to (m1);
    \node at (3,-2) [circle,draw,minimum size = 0pt, inner sep=0pt,] (p1) {$+$};
    \draw [-{Stealth[scale=2]}] (m1) to (p1);
    \draw [-{Stealth[scale=2]}] (wtm1) to (p1);
    \node at (6, -2) [circle,draw] (wt) {$\vect{w}_{t}$};
    \draw [-{Stealth[scale=2]}] (p1) to (wt);
    \node at (3,-4) [rectangle,draw] (h1) {${\color{olive}\gamma^{T-t}}\color{teal}\nabla_{\vect{w}_{t}}\lval(\vect{w}_t, \vect{\alpha})\vect{B}_t$};
    \draw [-{Stealth[scale=2]},dashed] (p1) to (h1);
    \node at (6,0) [rectangle,draw] (gt) {$\nabla_{\vect{w}_{t}}\ltrn(\vect{w}_{t}, \vect{\alpha})$};
    \node at (9,0) [circle,draw,minimum size = 0pt, inner sep=0pt,label=90:$-\eta$] (m2) {$\times$};
    \node at (9,-2) [circle,draw,minimum size = 0pt, inner sep=0pt,] (p2) {$+$};
    \node at (8,-4) [rectangle,draw] (h2) {${\color{olive}\gamma^{T-t-1}}\color{teal}\nabla_{\vect{w}_{t+1}}\lval(\vect{w}_{t+1}, \vect{\alpha})\vect{B}_{t+1}$};
    \draw [-{Stealth[scale=2]},dashed] (p2.south) to (p2.south|-h2.north);
    \draw [-{Stealth[scale=2]},] (wt) to (gt);
    \draw [-{Stealth[scale=2]},] (gt) to (m2);
    \draw [-{Stealth[scale=2]},] (m2) to (p2);
    \draw [-{Stealth[scale=2]},] (wt) to (p2);
    %summ
    \draw[draw=black,dashed] (11,-3) rectangle ++(-10.5, -2);
    \node at (-2,-3.5) [circle,draw] (summ) {$\Sigma$};
    \draw [-{Stealth[scale=2]},] (0.5,-3.5) -- (-1.6,-3.5);
    \node at (-2, -4.7) [rectangle, draw] (hyp) {$\color{violet}d_{\vect{\alpha}}\lval(\vect{w}_T, \vect{\alpha};\gamma)$};
    \draw [-{Stealth[scale=2]},] (summ) to (hyp);
    %imaginar lines
    \node at (-2, -2) [circle,] (aux) {};
    \draw [-{Stealth[scale=2]},dashed] (aux) to (wtm1);
\end{tikzpicture}
}
\end{figure}
\end{frame}

\begin{frame}{Обобщение метода $T1-T2$}
  \begin{block}{Определение} Аппроксимация гиперградиента, определяемая методом $T1-T2$ запишется как:
    \begin{align*}
        \hat{d}_{\vect{\alpha}}^{T1-T2}\lval(\vect{w}_T, \vect{\alpha}) = \nabla_{\vect{\alpha}}\lval(\vect{w}_T, \vect{\alpha}) + \nabla_{\vect{w}_T}\lval(\vect{w}_T, \vect{\alpha})\vect{B}_T.
    \end{align*}
    
  \end{block}
  \begin{block}{Теорема (Яковлев, 2024)}
    Пусть $\hat{d}_{\vect{\alpha}}(\vect{w}_T, \vect{\alpha}; \gamma)$ -- предложенная аппроксимация гиперградиента. Тогда имеет место следующий предел:
    \begin{align*}
        \lim_{\gamma \to 0^+}\hat{d}_{\vect{\alpha}}(\vect{w}_T, \vect{\alpha}; \gamma) = \nabla_{\vect{\alpha}}\lval(\vect{w}_T, \vect{\alpha}) + \nabla_{\vect{w}_T}\lval(\vect{w}_T, \vect{\alpha})\vect{B}_T.
    \end{align*}
    Таким образом, предложенный подход является обобщением $T1-T2$.
  \end{block}
  
\end{frame}


\begin{frame}{Достаточное условие спуска}
  \textbf{Предположения}
  \begin{enumerate}
    \item $\lval(., \vect{\alpha})$ является $L$-гладкой and $\mu$-сильно выпуклой для любого $\vect{\alpha}$.
   \item $\frac{\partial\vect{\Phi}(., \vect{\alpha})}{\partial\vect{\alpha}}$ является $C_B$-Липшицевой для любого $\vect{\alpha}$.
   \item $\|\frac{\partial\vect{\Phi}(\vect{w}, \vect{\alpha})}{\partial\vect{\alpha}}\| \leq B$ для любой пары $(\vect{w}, \vect{\alpha})$ для некоторого $B \geq 0$.
   \item $\vect{w}$ принадлежит некоторому выпуклому множеству с диаметром $D < \infty$.
   \item $\vect{\Phi}(\vect{w}, \vect{\alpha}) = \vect{w} - \eta\nabla_{\vect{w}}\ltrn(\vect{w}, \vect{\alpha})$ для некоторого $\eta \geq 0$.
   \item $\nabla^2_{\vect{w}}\ltrn(., \vect{\alpha}) = \vect{I}$ \, для любого $\vect{\alpha}$, а также $\nabla_{\vect{\alpha}}\lval(\vect{w}, \vect{\alpha}) = \vect{0}$ \, для любого $\vect{w}$.
   \item $\vect{B}_t\vect{B}_t^\top \succeq \kappa \vect{I}$ \, для некоторого $\kappa > 0$. 
   \item Определим $\vect{w}_{\infty} := \arg\min_{\vect{w}}\ltrn(\vect{w}, \vect{\alpha})$, $\vect{w}^*_2 := \arg\min_{\vect{w}}\lval(\vect{w}, \vect{\alpha})$. Пусть $\|\vect{w}_{\infty} - \vect{w}_2^*\| \geq 2De^{-\mu\eta T} + \delta$, для некоторого $\delta > 0$. 
\end{enumerate}

  \begin{block}{Теорема (Яковлев, 2024)}
  Пусть $\gamma = 1 - \eta \in (0, 1)$. Пусть также выполнены предположения (1-8), тогда найдется достаточно большое $T$ и универсальная константа $c > 0$ такая, что:
  \begin{align*}
      d_{\vect{\alpha}}\lval(\vect{w}_T, \vect{\alpha}) \hat{d}_{\vect{\alpha}}\lval(\vect{w}_T, \vect{\alpha}; \gamma)^\top \geq c \|d_{\vect{\alpha}}\lval(\vect{w}_T, \vect{\alpha})\|_2^2.
  \end{align*}
\end{block}
  
\end{frame}


\begin{frame}{Постановка вычислительного эксперимента}
  \begin{itemize}
    \item Цель -- сравнение качества предложенного подхода с существующими методами подсчета гиперградиента.
    \item Эксперимент проводится на задаче мета-обучения.
    \begin{minipage}{0.49\textwidth}
    \begin{align*}
      \vect{\alpha}^* &= \arg\min_{\vect{\alpha}}\mathbb{E}_{\mathcal{T}}\mathbb{E}_{\mathcal{S}|\mathcal{T}}\lval(\vect{w}^*, \vect{\alpha}; \mathcal{S}),\\
      \mathrm{s.t.} \quad \vect{w}^* &= \arg\min_{\vect{w}}\ltrn(\vect{w}, \vect{\alpha};\mathcal{S}).
    \end{align*}
    \end{minipage}%
    \begin{minipage}{0.49\textwidth}
      \centering
      \begin{tikzpicture}
        \draw[rounded corners=5pt,label=S,draw] (0,0) rectangle ++(0.6,1.5) node[pos=.5] (inp) {$\mathcal{S}$};
        \draw[rounded corners=5pt,label=S,draw] (1,0) rectangle ++(1.6,1.5) node[pos=.5] (model) {$\vect{\alpha}$};
        \draw[rounded corners=5pt,label=S,draw] (3,0) rectangle ++(0.6,1.5) node[pos=.5] (w) {$\vect{w}$};
        \draw node[circle,draw] at (4.5, 0.75) (out) {$\mathcal{L}$};
        \draw [-{Stealth[scale=1]},] (0.6, 0.75) -- (1.0, 0.75);
        \draw [-{Stealth[scale=1]},] (2.6, 0.75) -- (3.0, 0.75);
        \draw [-{Stealth[scale=1]},] (3.6, 0.75) -- (4.15, 0.75);
      \end{tikzpicture}
    \end{minipage}
    \item В сравнении участвуют следующие базовые методы:
  \end{itemize}
  \begin{small}
  \begin{align*}
    \textbf{(FO)}: \quad &
    \hat{d}_{\vect{\alpha}}^\text{FO}\lval(\vect{w}_T, \vect{\alpha}) = \nabla_{\vect{\alpha}}\lval(\vect{w}_T, \vect{\alpha}),\\
    \textbf{(IFT)}: \quad &
    \hat{d}_{\vect{\alpha}}^\text{IFT}\lval(\vect{w}_T, \vect{\alpha}) =
    \nabla_{\vect{\alpha}}\lval - \nabla_{\vect{w}}\lval\left(\sum_{j\leq i}\left[\vect{I} - \nabla^2_{\vect{w},\vect{w}}\ltrn\right]^j\right)\nabla^2_{\vect{w},\vect{\alpha}}\ltrn\bigg\vert_{(\vect{w}_T, \vect{\alpha})},\\
    \textbf{(T1-T2)}: \quad &\hat{d}_{\vect{\alpha}}^{T1-T2}\lval(\vect{w}_T, \vect{\alpha}) = \nabla_{\vect{\alpha}}\lval(\vect{w}_T, \vect{\alpha}) + \nabla_{\vect{w}_T}\lval(\vect{w}_T, \vect{\alpha})\vect{B}_T.
  \end{align*}
  \end{small}

  
\end{frame}


\begin{frame}{Результаты вычислительного эксперимента}
  \begin{itemize}
    \item Рассматриваются задачи классификации на $n$ классов с $k$ примерами на каждый класс ($n$-way, $k$-shot).
    \item  Приводится точность предсказаний на мета-контроле,
    а также вычислительная сложность итерации подсчета гиперградиента.
  \end{itemize}
  \begin{table}
    \centering
    \begin{tabular}{c|c|ccc}
    \toprule
    \textbf{Method} & \textbf{\#JVPs} & \textbf{3-way, 10-shot} & \textbf{4-way, 10-shot} & \textbf{5-way, 10-shot} \\ \midrule
    FO & 0 & 43.48 $\pm$ 0.69 & 34.15 $\pm$ 0.53 & 28.59 $\pm$ 0.47 \\
    $T1-T2$ & 1 & 42.96 $\pm$ 0.79 & 33.95 $\pm$ 0.64 & 27.59 $\pm$ 0.46 \\
    IFT & 11 & 40.14 $\pm$ 0.73 & 33.23 $\pm$ 0.41 & 27.20 $\pm$ 0.52 \\
    Ours ($\gamma = 0.99$) & 10 & \textbf{46.10} $\pm$ \textbf{0.82} & \textbf{36.94} $\pm$ \textbf{1.07} & \textbf{29.79} $\pm$ \textbf{0.62} \\ \bottomrule
    \end{tabular}
  \end{table}
Из таблицы видно, что предложенный метод превосходит существующие методы градиентной оптимизации гиперпараметров в терминах 
точности предсказаний на мета-контроле, имея сопоставимые вычислительные затраты.
  
\end{frame}



\begin{frame}{Выносится на защиту}
    % \begin{itemize}
    % \item Рассмотрена задача поиска архитектуры модели глубокого
    % обучения на мультидоменных данных. Задача рассматривалась как
    % задача мультимоделирования.
    % \item Предложены два метода регуляризации: регуляризация структуры и регуляризация
    % пространства скрытых представлений модели.
    % \item Продемонстрирована работоспособность предлагаемого решения. При использовании
    % первого регуляризатора мультимодель имеет меньшее число параметров. При использовании
    % второго регуляризатора модель имеет лучшую точность классификации.
    % \item В дальнейшем планируется провести вычислительный эксперимент на задаче
    % мультиязычного языкового моделирования.
    % \end{itemize}
    \begin{itemize}
      \item Рассмотрена задача оптимизации гиперпараметров.
      \item Предложен метод оптимизации гиперпараметров, удовлетворяющий одновременно трем условиям:
      \begin{itemize}
        \item онлайн оптимизация
        \item отсутствие смещения из-за короткого горизонта
        \item линейная сложность итерации и затраты памяти.
      \end{itemize}
      \item Продемонстрирована работоспособность предлагаемого решения.
      \item Проведен теоретический анализ предложенного метода.
    \end{itemize}
\end{frame}


\begin{frame}{Список публикаций}
  \begin{small}
  \begin{itemize}
    \item \textbf{(core-A*)} Yakovlev K. et al. GEC-DePenD: Non-Autoregressive Grammatical Error Correction with Decoupled Permutation and Decoding //Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). – 2023. – С. 1546-1558.
    \item \textbf{(core-A*)} Yakovlev K. et al. Sinkhorn Transformations for Single-Query Postprocessing in Text-Video Retrieval //Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. – 2023. – С. 2394-2398.
    \item Yakovlev K. D. et al. Neural Architecture Search with Structure Complexity Control //International Conference on Analysis of Images, Social Networks and Texts. – Cham : Springer International Publishing, 2021. – С. 207-219.
  \end{itemize}
  \end{small}
\end{frame}


\begin{frame}{Выступления на конференциях}
  \begin{itemize}
  \item Яковлев К.Д. Обобщенная жадная градиентная оптимизация гиперпараметров. //Труды
  66-й Всероссийской
  научной конференции
  МФТИ. 2024.
  \item Яковлев К.Д. Поиск согласованных нейросетевых моделей в задаче мультидоменного обучения. //Труды
  65-й Всероссийской
  научной конференции
  МФТИ в честь 115-летия
  Л.Д. Ландау. - 2023.
  \item Яковлев К.Д., Гребенькова О.С., Бахтеев О.Ю., Стрижов В.В. Выбор архитектуры модели с контролем сложности //
  Труды
  64-й Всероссийской
  научной конференции
  МФТИ. - 2021.
  \end{itemize}
  
\end{frame}

%----------------------------------------------------------------------------------------------------------
\end{document} 