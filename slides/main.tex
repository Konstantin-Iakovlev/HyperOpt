\documentclass[aspectratio=169]{beamer}
\beamertemplatenavigationsymbolsempty
\usecolortheme{beaver}
\setbeamertemplate{blocks}[rounded=true, shadow=true]
\setbeamertemplate{footline}[page number]


\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage[english,russian]{babel}
\usepackage{xcolor}



\usepackage[utf8]{inputenc}
\usepackage{amssymb,amsfonts,amsmath,mathtext}
%\usepackage{subfig}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage[all]{xy} % xy package for diagrams
\usepackage{array}
\usepackage{multicol}% many columns in slide
\usepackage{hyperref}% urls
\usepackage{hhline}%tables
\usepackage{booktabs}
% \usepackage{biblatex}



\def\bw{\mathbf{w}}
\def\balpha{\boldsymbol{\alpha}}
\def\ltrn{\mathcal{L}_{\mathrm{train}}}
\def\lval{\mathcal{L}_{\mathrm{val}}}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}


\definecolor{dark_green}{rgb}{0, 0.788, 0}
\definecolor{dark_red}{rgb}{0.9, 0, 0}


% Your figures are here:
\graphicspath{ {fig/} {../fig/} }

\definecolor{ao(english)}{rgb}{0.0, 0.5, 0.0}
\definecolor{bleudefrance}{rgb}{0.19, 0.55, 0.91}

%----------------------------------------------------------------------------------------------------------
% \title[\hbox to 56mm{Feature generation}]{Поиск согласованных нейросетевых моделей в задаче мультидоменного обучения}
\title[\hbox to 56mm{Feature generation}]{Обобщенная жадная градиентная оптимизация гиперпараметров}
% \author{К.\,Д.~Яковлев\inst{1} \and \and О.\,Ю.~Бахтеев\inst{1,2}\and В.\,В.~Стрижов\inst{1,2} \\
% \tt{\footnotesize \{iakovlev.kd, bakhteev, strijov\}@phystech.edu }}
% \institute{\inst{1} Москва, Московский физико-технический институт \and
% \inst{2} Москва, Вычислительный центр им. А.А. Дородницына ФИЦ ИУ РАН} \date{2023}
\author{К.\,Д.~Яковлев\inst{} \\
\tt{\footnotesize iakovlev.kd@phystech.edu }}
\institute{\inst{} Москва, Московский физико-технический институт \\
\textbf{Научный руководитель}: к.ф.-м.н. Бахтеев Олег Юрьевич} \date{2024}
%----------------------------------------------------------------------------------------------------------
\begin{document}
%----------------------------------------------------------------------------------------------------------
\begin{frame}
\thispagestyle{empty}
\maketitle
\end{frame}
%-----------------------------------------------------------------------------------------------------
\begin{frame}{Цель исследования}

\begin{block}{Цель} 
  Предложить градиентный метод оптимизации гиперпараметров с линейейной по количеству параметров и гиперпараметров
  сложностью итерации и затратами памяти.
\end{block}

~\\
\begin{block}{Проблема}
  Существующие методы не гарантируют выполнения следующих условий одновременно: 1) отсутствие требований на сходимость
  внутренней процедуры оптимизации к единственному решению,
  2) отсутствие смещения из-за короткого горизонта, 3) линейная сложность итерации и затраты памяти.
\end{block}
~\\
\begin{block}{Метод решения}
  Предлагаемый метод основан на агрегации жадных гиперградиентов без дополнительных вычислительных затрат.
\end{block}

\end{frame}


\begin{frame}{Агрегация жадных гиперградиентов}
  Пусть задано $\gamma \in (0, 1)$. Тогда аппроксимация гиперградиента запишется как:
  \begin{align*}
    \hat{d}_{\boldsymbol{\alpha}}\lval(\vect{w}_T, \vect{\alpha}; \gamma) = \frac{\partial}{\partial \boldsymbol{\alpha}}\mathcal{L}_2(\mathbf{w}_T(\boldsymbol{\alpha}), \boldsymbol{\alpha}) +
    \sum_{t=1}^T \mathbf{B}_t\frac{\partial\mathcal{L}_2(\mathbf{w}_t, \boldsymbol{\alpha})}{\partial \mathbf{w}_t}\gamma^{T-t}.
  \end{align*}
  \begin{table}
    \begin{tabular}{c|c|c|c|c|c} \toprule
       & IFT & RMD & DrMAD & $T1-T2$ & Ours \\ \midrule
      Онлайн оптимизация & \color{dark_red}{\texttimes} & \color{dark_green}{\checkmark} & \color{dark_red}{\texttimes} & \color{dark_green}{\checkmark} & \color{dark_green}{\checkmark} \\
      Длинный горизонт & \color{dark_green}{\checkmark} & \color{dark_green}{\checkmark} & \color{dark_green}{\checkmark} & \color{dark_red}{\texttimes} & \color{dark_green}{\checkmark} \\
      Линейная сложность & \color{dark_green}{\checkmark} & \color{dark_red}{\texttimes} & \color{dark_green}{\checkmark} & \color{dark_green}{\checkmark} & \color{dark_green}{\checkmark} \\ \bottomrule
      
    \end{tabular}
  \end{table}
  
\end{frame}

%----------------------------------------------------------------------------------------------------------


% \begin{frame}{Основная литература}
% \begin{thebibliography}{1}


% \bibitem{darts} 
% Hanxiao Liu and Karen Simonyan and Yiming Yang. 
% \textit{DARTS: Differentiable Architecture Search}.
% CoRR, 2018.


% \bibitem{wang2021multi}
% Wang, Q., Ke, J., Greaves, J., Chu, G.,
% Bender, G., Sbaiz, L., Go, A., Howard, A., Yang, M.,
% Gilbert, J. \& Others
% \textit{Multi-path neural networks for on-device multi-domain visual classification}.
% CoRR, 2021.

% \bibitem{darts-cc}
% Yakovlev, K., Grebenkova, O., Bakhteev, O. \& Strijov, V.
% \textit{Neural Architecture Search with Structure Complexity Control}.
% CoRR, 2022.

% \end{thebibliography}	
% \end{frame}

\begin{frame}{Постановка задачи оптимизации гиперпараметров}
\begin{itemize}
  \item Пусть задан вектор параметров модели $\mathbf{w} \in \mathbb{R}^p$ и вектор гиперпараметров
  $\boldsymbol{\alpha} \in \mathbb{R}^h$. Задача оптимизации:
  \begin{align*}
    \boldsymbol{\alpha}^* &= \arg\min_{\boldsymbol{\alpha}}\mathcal{L}_2(\mathbf{w}^*, \boldsymbol{\alpha}), \\
    \mathrm{s.t.} \quad \mathbf{w}^* &= \arg\min_{\mathbf{w}}\mathcal{L}_1(\mathbf{w}, \boldsymbol{\alpha}).
  \end{align*}
  \item Пусть внутренняя задача решается с помощью оптимизатора $\mathbf{\Phi}(., .)$:
  \begin{align*}
    \mathbf{w}_{t + 1}(\boldsymbol{\alpha}) = \mathbf{\Phi}(\mathbf{w}_t, \boldsymbol{\alpha}), \quad
    t = \overline{1, T}.
  \end{align*}
  \item Гиперградиент запишется как:
  \begin{align*}
    &d_{\boldsymbol{\alpha}}\mathcal{L}_2(\mathbf{w}_T(\boldsymbol{\alpha}), \boldsymbol{\alpha}) = 
    \frac{\partial}{\partial \boldsymbol{\alpha}}\mathcal{L}_2(\mathbf{w}_T(\boldsymbol{\alpha}), \boldsymbol{\alpha}) + 
    \sum_{t=1}^T\mathbf{B}_t\mathbf{A}_{t+1}\ldots\mathbf{A}_T
    \frac{\partial \mathcal{L}_2(\mathbf{w}_T(\boldsymbol{\alpha}), \boldsymbol{\alpha})}{\partial\mathbf{w}}, \\
    &\mathbf{B}_t = \frac{\partial\mathbf{\Phi}(\mathbf{w}_{t-1}, \boldsymbol{\alpha})}{\partial\boldsymbol{\alpha}},
    \quad \mathbf{A}_t = \frac{\partial\mathbf{\Phi}(\mathbf{w}_{t-1}, \boldsymbol{\alpha})}{\partial\boldsymbol{w}_{t - 1}}.
  \end{align*}
\end{itemize}
\end{frame}


\begin{frame}{Аппроксимация гиперградиента}
  Пусть задано $\gamma \in (0, 1)$. Тогда аппроксимация гиперградиента запишется как:
  \begin{align*}
    \hat{d}_{\boldsymbol{\alpha}}\lval(\vect{w}_T, \vect{\alpha}; \gamma) = \frac{\partial}{\partial \boldsymbol{\alpha}}\mathcal{L}_2(\mathbf{w}_T(\boldsymbol{\alpha}), \boldsymbol{\alpha}) +
    \sum_{t=1}^T \mathbf{B}_t\frac{\partial\mathcal{L}_2(\mathbf{w}_t, \boldsymbol{\alpha})}{\partial \mathbf{w}_t}\gamma^{T-t}.
  \end{align*}
  \begin{block}{Теорема (Яковлев, 2024)}
    Пусть $\hat{d}_{\vect{\alpha}}(\vect{w}_T, \vect{\alpha}; \gamma)$ -- предложенная аппроксимация гиперградиента. Тогда имеет место следующий предел:
    \begin{align}
        \lim_{\gamma \to 0^+}\hat{d}_{\vect{\alpha}}(\vect{w}_T, \vect{\alpha}; \gamma) = \nabla_{\vect{\alpha}}\lval(\vect{w}_T, \vect{\alpha}) + \nabla_{\vect{w}_T}\lval(\vect{w}_T, \vect{\alpha})\vect{B}_T.
    \end{align}
    Таким образом, предложенный подход является обобщением $T1-T2$.
  \end{block}
\end{frame}


\begin{frame}{Достаточное условие спуска}
  \textbf{Предположения}
  \begin{enumerate}
    \item $\lval(., \vect{\alpha})$ является $L$-гладкой and $\mu$-сильно выпуклой для любого $\vect{\alpha}$.
   \item $\frac{\partial\vect{\Phi}(., \vect{\alpha})}{\partial\vect{\alpha}}$ является $C_B$-Липшицевой для любого $\vect{\alpha}$.
   \item $\|\frac{\partial\vect{\Phi}(\vect{w}, \vect{\alpha})}{\partial\vect{\alpha}}\| \leq B$ для любой пары $(\vect{w}, \vect{\alpha})$ для некоторого $B \geq 0$.
   \item $\vect{w}$ принадлежит некоторому выпуклому множеству с диаметром $D < \infty$.
   \item $\vect{\Phi}(\vect{w}, \vect{\alpha}) = \vect{w} - \eta\nabla_{\vect{w}}\ltrn(\vect{w}, \vect{\alpha})$ для некоторого $\eta \geq 0$.
   \item $\nabla^2_{\vect{w}}\ltrn(., \vect{\alpha}) = \vect{I}$ \, для любого $\vect{\alpha}$, а также $\nabla_{\vect{\alpha}}\lval(\vect{w}, \vect{\alpha}) = \vect{0}$ \, для любого $\vect{w}$.
   \item $\vect{B}_t\vect{B}_t^\top \succeq \kappa \vect{I}$ \, для некоторого $\kappa > 0$. 
   \item Определим $\vect{w}_{\infty} := \arg\min_{\vect{w}}\ltrn(\vect{w}, \vect{\alpha})$, $\vect{w}^*_2 := \arg\min_{\vect{w}}\lval(\vect{w}, \vect{\alpha})$. Пусть $\|\vect{w}_{\infty} - \vect{w}_2^*\| \geq 2De^{-\mu\eta T} + \delta$, для некоторого $\delta > 0$. 
\end{enumerate}

  \begin{block}{Теорема (Яковлев, 2024)}
  Пусть $\gamma = 1 - \eta \in (0, 1)$. Пусть также выполнены предположения (1-8), тогда найдется достаточно большое $T$ и универсальная константа $c > 0$ такая, что:
  \begin{align*}
      d_{\vect{\alpha}}\lval(\vect{w}_T, \vect{\alpha}) \hat{d}_{\vect{\alpha}}\lval(\vect{w}_T, \vect{\alpha}; \gamma)^\top \geq c \|d_{\vect{\alpha}}\lval(\vect{w}_T, \vect{\alpha})\|_2^2.
  \end{align*}
\end{block}
  
\end{frame}



% \begin{frame}{Постановка вычислительного эксперимента 1}
%   \begin{itemize}
%     \item Цель -- сравнение качества предложенного подхода с существующими методами подсчета гиперградиента.
%     \item Эксперимент проводится на задаче очистки обучающей выборки. Приводится точность предсказания на отложенной выборке.
%     \item Сравниваются следующие методы: DrMAD, IFT, Greedy.
%   \end{itemize}


%   \begin{minipage}{0.5\textwidth}
%   \begin{table}[h]
%     \centering
%     \begin{tabular}{c|c|c|c} \toprule
%       \textbf{Method}/ \textbf{Noise} & 0.3 & 0.5 & 0.7 \\ \midrule
%       Naive & 28.94 & 25.17 & 19.23 \\
%       IFT(2) & 28.97 & 25.81 & 20.89 \\
%       Greedy & 30.36 & 26.76 & 21.38 \\
%       Ours ($\gamma=0.99$) & \textbf{30.39} & \textbf{26.77} & \textbf{21.47} \\ \bottomrule
%         % \textbf{Method} & \textbf{Valid. Acc.} & $\#$\textbf{JVPs} \\ \hline
%         % Truncated backpropagation (Lukethina) & 72.5 & $1 \; (1)$ \\
%         % DrMAD & 69.8 & $99 \; (2T - 1)$ \\
%         % IFT(9, 5) & 70.3 & $50 \;  ((N + 1)K)$ \\
%         % IFT(4, 10) & 70.7 & $50 \; ((N + 1)K)$ \\
%         % Proposed ($\gamma = 0.99$) & $\textbf{73.5}^*$ & $50 \; (T)$ \\ \hline
%     \end{tabular}
%   \end{table}
%   \end{minipage}\hfil
%   \begin{minipage}{0.5\textwidth}
%     \begin{align*}
%       \boldsymbol{\alpha}^* &= \arg\min_{\boldsymbol{\alpha}}\mathcal{L}_\text{val}(\boldsymbol{w}^*), \\
%       \mathrm{s.t.} \quad \mathbf{w}^* &= \arg\min_{\mathbf{w}}\sum_{i=1}^N\sigma(\alpha_i)\ell_i(\mathbf{w}).
%     \end{align*}
%   \end{minipage}

% Из таблицы видно, что предложенный метод превосходит существующие методы оптимизации гиперпараметров в терминах 
% точности предсказаний на отложенной выборке, имея сопоставимые вычислительные затраты.
% \end{frame}


\begin{frame}{Постановка вычислительного эксперимента}
  \begin{itemize}
    \item Цель -- сравнение качества предложенного подхода с существующими методами подсчета гиперградиента.
    \item Эксперимент проводится на задаче мета-обучения. Приводится точность предсказаний на мета-контроле,
    а также вычислительная сложность итерации подсчета гиперградиента.
    \item Сравниваются следующие методы: FO, IFT, $T1-T2$.
  \end{itemize}
  \begin{table}
    \centering
    \begin{tabular}{c|c|ccc}
    \toprule
    \textbf{Method} & \textbf{\#JVPs} & \textbf{3-way, 10-shot} & \textbf{4-way, 10-shot} & \textbf{5-way, 10-shot} \\ \midrule
    FO & 0 & 43.48 $\pm$ 0.69 & 34.15 $\pm$ 0.53 & 28.59 $\pm$ 0.47 \\
    $T1-T2$ & 1 & 42.96 $\pm$ 0.79 & 33.95 $\pm$ 0.64 & 27.59 $\pm$ 0.46 \\
    IFT & 11 & 40.14 $\pm$ 0.73 & 33.23 $\pm$ 0.41 & 27.20 $\pm$ 0.52 \\
    Ours ($\gamma = 0.99$) & 10 & \textbf{46.10} $\pm$ \textbf{0.82} & \textbf{36.94} $\pm$ \textbf{1.07} & \textbf{29.79} $\pm$ \textbf{0.62} \\ \bottomrule
    \end{tabular}
\end{table}

Из таблицы видно, что предложенный метод превосходит существующие методы градиентной оптимизации гиперпараметров в терминах 
точности предсказаний на мета-контроле, имея сопоставимые вычислительные затраты.
  
\end{frame}





% \begin{frame}{Постановка вычислительного эксперимента}

% \begin{itemize}
% \item Цель -- получение зависимости качества работы мультимодели и
% количества ее параметров в зависимости от используемого регуляризатора.
% \item Эксперимент проводится на подвыборке MNIST. В качестве доменов рассматриваются
% изображения, повернутые на угол, кратный $\pi/2$. Число доменов меняется от 1 до 4.
%  Сравниваются следующие модели:
% мультимодель со структурной регуляризацией, мультимодель с регуляризацией скрытых представлений,
% а также модель, не учитывающая разбиение выборки на домены.
% \item Оценивается средняя точность (accuracy) на тестовой выборке для каждого из доменов.
% Также приводится количество параметров для каждой модели.
% \end{itemize}

% \end{frame}


% \begin{frame}{Результаты вычислительного эксперимента}
%   \begin{table}[h!]
%     \centering
%      \begin{tabular}{||c c c||}
%      \hline
%      model & accuracy & num. of params \\ [0.5ex] \hline\hline
%      \multicolumn{3}{|| c ||}{1 domain} \\
%       single & 60.59 & 5029 \\ \hline \hline 

%      \multicolumn{3}{|| c ||}{2 domains} \\
%      single, union & 66.95 & 6560 \\
%      multimodel, struct & 62.86 & \textbf{5248} \\
%      multimodel, contr & \textbf{69.64} & 9328 \\
%      \hline \hline

%      \multicolumn{3}{|| c ||}{3 domains} \\
%      single, union & 63.02 & 5685\\
%      multimodel, struct & 64.85 & \textbf{6826}\\
%      multimodel, contr & \textbf{65.01} & 12096 \\
%      \hline\hline

%      \multicolumn{3}{|| c ||}{4 domains} \\
%      single, union & 67.16 & 6560\\
%      multimodel, struct & 63.15 & \textbf{7872}\\
%      multimodel, contr & \textbf{67.98} & 13685 \\
%      \hline\hline
%      \end{tabular}
%     \end{table}
% \end{frame}


% \begin{frame}{Вычислтельный эксперимент на задче языкового моделирования}
%   \begin{enumerate}
%     \item Задача языкового моделирования решалась на выборке IWSLT14.
%     \item Расматривались одиночные модели, решающие задачу на одном домене и на
%     объединении доменов, а также предлагаемая мультимодель.
%   \end{enumerate}
%   \begin{table}
%     \centering
%     \begin{tabular}{|c|c|c|}
%       \hline
%       \textbf{Model} & \textbf{En, ppl} & \textbf{De, ppl} \\ \hline
%       Single, En & \textbf{59.35} & - \\ 
%       Single, De & - & - \\ \hline
%       Single, En+De & 72.68 & 82.15 \\
%       Multimodel & \textbf{64.91} & \textbf{74.06} \\ \hline
%     \end{tabular}
%   \end{table} 
% \end{frame}



\begin{frame}{Выносится на защиту}
    % \begin{itemize}
    % \item Рассмотрена задача поиска архитектуры модели глубокого
    % обучения на мультидоменных данных. Задача рассматривалась как
    % задача мультимоделирования.
    % \item Предложены два метода регуляризации: регуляризация структуры и регуляризация
    % пространства скрытых представлений модели.
    % \item Продемонстрирована работоспособность предлагаемого решения. При использовании
    % первого регуляризатора мультимодель имеет меньшее число параметров. При использовании
    % второго регуляризатора модель имеет лучшую точность классификации.
    % \item В дальнейшем планируется провести вычислительный эксперимент на задаче
    % мультиязычного языкового моделирования.
    % \end{itemize}
    \begin{itemize}
      \item Рассмотрена задача оптимизации гиперпараметров.
      \item Предложен метод оптимизации гиперпараметров, удовлетворяющий одновременно трем условиям:
      \begin{itemize}
        \item онлайн оптимизация
        \item отсутствие смещения из-за короткого горизонта
        \item линейная сложность итерации и затраты памяти.
      \end{itemize}
      \item Продемонстрирована работоспособность предлагаемого решения.
      \item Проведен теоретический анализ предложенного метода.
    \end{itemize}
\end{frame}


% \begin{frame}{Список публикаций}
%   \begin{itemize}
%     \item \textbf{(core-A*)} Yakovlev K. et al. GEC-DePenD: Non-Autoregressive Grammatical Error Correction with Decoupled Permutation and Decoding //Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). – 2023. – С. 1546-1558.
%     \item \textbf{(core-A*)} Yakovlev K. et al. Sinkhorn Transformations for Single-Query Postprocessing in Text-Video Retrieval //Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. – 2023. – С. 2394-2398.
%     \item Yakovlev K. D. et al. Neural Architecture Search with Structure Complexity Control //International Conference on Analysis of Images, Social Networks and Texts. – Cham : Springer International Publishing, 2021. – С. 207-219.
%   \end{itemize}
% \end{frame}

%----------------------------------------------------------------------------------------------------------
\end{document} 