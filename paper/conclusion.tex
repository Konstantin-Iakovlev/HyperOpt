\section{Conclusion}
The paper presents an approximation of the true hypergradient for gradient-based bilevel optimization that avoids the high memory cost and short horizon bias. Additionally, the method does not require the assumption of convergence to an optimal solution for the inner optimization. The proposed method exploits an aggregation of greedy gradients calculated at each step of the inner trajectory. Our theoretical findings suggest that the approximation satisfies the sufficient descent condition. Empirically, the introduced method outperforms the baselines in terms of validation accuracy, having comparable computational costs. One promising direction for future research is to investigate more accurate Hessian approximations.
