{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53d268d8-393e-4b79-947e-aed2ca5baf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/konstantinakovlev/jax-metal/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/konstantinakovlev/jax-metal/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "\n",
    "from clu import metrics\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "from flax import struct, core\n",
    "import optax\n",
    "\n",
    "import jax_dataloader as jdl\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from typing import Callable, Any, List\n",
    "\n",
    "import torch, torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import glob\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 16\n",
    "BIGGER_SIZE = 12\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ba779d-bea7-46f0-87dd-0c7fab7b4423",
   "metadata": {},
   "source": [
    "### Build DARTS cell on Haiku\n",
    "\n",
    "Useful example: https://github.com/deepmind/dm-haiku/blob/main/examples/transformer/model.py\n",
    "\n",
    "\n",
    "## OPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ada59acd-d5c3-4279-bc88-297c2564d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(hk.Module):\n",
    "    def __init__(self, p=0.0):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, x, is_training=False):\n",
    "        if is_training and self.p > 0:\n",
    "            keep_p = 1.0 - self.p\n",
    "            mask = jax.random.bernoulli(hk.next_rng_key(), keep_p, (x.shape[0], 1, 1, 1))\n",
    "            return x / keep_p * mask\n",
    "        return x\n",
    "\n",
    "\n",
    "class Identity(hk.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, x, is_training=False):\n",
    "        return x\n",
    "\n",
    "\n",
    "class PoolBN(hk.Module):\n",
    "    def __init__(self, pool_type, kernel_size, stride, affine=True):  # C is not needed\n",
    "        super().__init__()\n",
    "        if pool_type.lower() == 'max':\n",
    "            self.pool = hk.MaxPool(window_shape=kernel_size, strides=stride, padding='SAME')\n",
    "        elif pool_type.lower() == 'avg':\n",
    "            self.pool = hk.AvgPool(window_shape=kernel_size, strides=stride, padding='SAME') # TODO: count_include_pad=False\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        self.bn = hk.BatchNorm(affine, affine, 0.9)\n",
    "\n",
    "    def __call__(self, x, is_training):\n",
    "        out = self.pool(x)\n",
    "        out = self.bn(out, is_training)\n",
    "        return out\n",
    "\n",
    "\n",
    "class StdConv(hk.Module):\n",
    "    def __init__(self, C_out, kernel_size, stride, affine=True):  # C_in is not needed\n",
    "        super().__init__()\n",
    "        self.conv = hk.Conv2D(C_out, kernel_shape=kernel_size, stride=stride,\n",
    "                              padding='SAME', with_bias=False)\n",
    "        self.bn = hk.BatchNorm(affine, affine, 0.9)\n",
    "\n",
    "    def __call__(self, x, is_training):\n",
    "        out = jax.nn.relu(x)\n",
    "        out = self.conv(out)\n",
    "        out = self.bn(out, is_training)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FacConv(hk.Module):\n",
    "    \"\"\"\n",
    "    Factorized conv: ReLU - Conv(Kx1) - Conv(1xK) - BN\n",
    "    \"\"\"\n",
    "    def __init__(self, C_in, C_out, kernel_length, stride, affine=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = hk.Conv2D(C_in, kernel_shape=(kernel_length, 1), stride=stride, padding='SAME',\n",
    "                              with_bias=False)\n",
    "        self.conv2 = hk.Conv2D(C_out, kernel_shape=(1, kernel_length), stride=stride, padding='SAME',\n",
    "                               with_bias=False)\n",
    "        self.bn = hk.BatchNorm(affine, affine, 0.9)\n",
    "        \n",
    "    def __call__(self, x, is_training):\n",
    "        out = jax.nn.relu(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn(out, is_training)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DilConv(hk.Module):\n",
    "    \"\"\"\n",
    "    (Dilated) depthwise separable conv.\n",
    "    ReLU - (Dilated) depthwise separable - Pointwise - BN.\n",
    "    If dilation == 2, 3x3 conv => 5x5 receptive field, 5x5 conv => 9x9 receptive field.\n",
    "    \"\"\"\n",
    "    def __init__(self, C_in, C_out, kernel_size, stride, dilation, affine=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = hk.Conv2D(C_in, kernel_shape=kernel_size, stride=stride, rate=dilation,\n",
    "                               padding='SAME', with_bias=False)\n",
    "        self.conv2 = hk.Conv2D(C_out, kernel_shape=1, stride=1, rate=dilation,\n",
    "                               padding='SAME', with_bias=False)\n",
    "        self.bn = hk.BatchNorm(affine, affine, 0.9)\n",
    "        \n",
    "    def __call__(self, x, is_training):\n",
    "        out = jax.nn.relu(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn(out, is_training)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SepConv(hk.Module):\n",
    "    \"\"\"\n",
    "    Depthwise separable conv.\n",
    "    DilConv(dilation=1) * 2.\n",
    "    \"\"\"\n",
    "    def __init__(self, C_in, C_out, kernel_size, stride, affine=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = DilConv(C_in, C_out, kernel_size, stride, 1, affine)\n",
    "        self.conv2 = DilConv(C_in, C_out, kernel_size, 1, 1, affine)\n",
    "\n",
    "    def __call__(self, x, is_training):\n",
    "        return self.conv2(self.conv1(x, is_training), is_training)\n",
    "        \n",
    "\n",
    "class FactorizedReduce(hk.Module):\n",
    "    \"\"\"\n",
    "    Reduce feature map size by factorized pointwise (stride=2).\n",
    "    \"\"\"\n",
    "    def __init__(self, C_out, affine=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = hk.Conv2D(C_out // 2, kernel_shape=1, stride=2, padding='VALID', with_bias=False)\n",
    "        self.conv2 = hk.Conv2D(C_out // 2, kernel_shape=1, stride=2, padding='VALID', with_bias=False)\n",
    "        self.bn = hk.BatchNorm(affine, affine, 0.9)\n",
    "\n",
    "    def __call__(self, x, is_training):\n",
    "        out = jax.nn.relu(x)\n",
    "        out = jnp.concatenate([self.conv1(out), self.conv2(out[:, 1:, 1:])], axis=-1)\n",
    "        out = self.bn(out, is_training)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "        \n",
    "### TEST\n",
    "def test_dp():\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    x = jnp.ones((3, 2, 1, 1))\n",
    "    dp = hk.transform(lambda x, is_training: DropPath(0.5)(x, is_training))\n",
    "    params = dp.init(rng, x, True)\n",
    "    assert dp.apply(params, rng, x, True).shape == x.shape\n",
    "\n",
    "\n",
    "def test_id():\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    x = jnp.ones((3, 2, 1, 1))\n",
    "    id_ = hk.transform(lambda x, is_training: Identity()(x, is_training))\n",
    "    params = id_.init(rng, x, True)\n",
    "    assert id_.apply(params, None, x, True).shape == x.shape\n",
    "\n",
    "\n",
    "def test_pool():\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    x = jnp.ones((3, 16, 16, 3))\n",
    "    pool = hk.transform_with_state(lambda x, is_t: PoolBN('avg', 3, 1)(x, is_t))\n",
    "    params, state = pool.init(rng, x, True)\n",
    "    assert pool.apply(params, state, rng, x, True)[0].shape == x.shape  # out, state_new = apply    \n",
    "\n",
    "\n",
    "def test_std_conv():\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    x = jnp.ones((3, 16, 16, 3))\n",
    "    conv = hk.transform_with_state(lambda x, is_t: StdConv(3, 3, 1)(x, is_t))\n",
    "    params, state = conv.init(rng, x, True)\n",
    "    assert conv.apply(params, state, rng, x, True)[0].shape == x.shape  # out, state_new = apply\n",
    "\n",
    "def test_fac_conv():\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    x = jnp.ones((3, 16, 16, 3))\n",
    "    conv = hk.transform_with_state(lambda x, is_t: FacConv(3, 3, 3, 1)(x, is_t))\n",
    "    params, state = conv.init(rng, x, True)\n",
    "    assert conv.apply(params, state, rng, x, True)[0].shape == x.shape  # out, state_new = apply \n",
    "\n",
    "\n",
    "def test_dil_conv():\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    x = jnp.ones((3, 32, 32, 16))\n",
    "    conv = hk.transform_with_state(lambda x, is_t: DilConv(16, 16, 5, 1, 2, False)(x, is_t))\n",
    "    params, state = conv.init(rng, x, True)  # out, state_new = apply\n",
    "    out = conv.apply(params, state, rng, x, True)[0]\n",
    "    assert out.shape == x.shape, f'{out.shape}, {x.shape}'\n",
    "\n",
    "\n",
    "def test_sep_conv():\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    x = jnp.ones((3, 32, 32, 16))\n",
    "    conv = hk.transform_with_state(lambda x, is_t: SepConv(16, 16, 3, 1)(x, is_t))\n",
    "    params, state = conv.init(rng, x, True)\n",
    "    assert conv.apply(params, state, rng, x, True)[0].shape == x.shape  # out, state_new = apply \n",
    "\n",
    "\n",
    "def test_fact_reduce():\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    x = jnp.ones((3, 16, 16, 3))\n",
    "    conv = hk.transform_with_state(lambda x, is_t: FactorizedReduce(4)(x, is_t))\n",
    "    params, state = conv.init(rng, x, True)\n",
    "    out = conv.apply(params, state, rng, x, True)[0]  # out, state_new = apply \n",
    "    assert out.shape == (3, 8, 8, 4), f'{out.shape}'\n",
    "\n",
    "\n",
    "\n",
    "test_dp()\n",
    "test_id()\n",
    "test_pool()\n",
    "test_std_conv()\n",
    "test_fac_conv()\n",
    "test_dil_conv()\n",
    "test_sep_conv()\n",
    "test_fact_reduce()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf16ed5-1fc9-4df9-ad81-82c4d388155a",
   "metadata": {},
   "source": [
    "## Node\n",
    "\n",
    "https://github.com/microsoft/nni/blob/master/examples/nas/legacy/oneshot/darts/model.py\n",
    "\n",
    "See also an unresolved issue\n",
    "\n",
    "https://stackoverflow.com/questions/76147761/train-multiple-nn-in-parallel-with-jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4513a23-06ca-4437-9ef8-32bc84121177",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerChoice(hk.Module):\n",
    "    def __init__(self, channels, stride, label='none'):\n",
    "        super().__init__()\n",
    "        self.label = label\n",
    "        self.ops = [\n",
    "            PoolBN('max', 3, stride, affine=False),\n",
    "            PoolBN('avg', 3, stride, affine=False),\n",
    "            Identity() if stride == 1 else FactorizedReduce(channels, False),\n",
    "            SepConv(channels, channels, 3, stride, False),\n",
    "            SepConv(channels, channels, 5, stride, False),\n",
    "            DilConv(channels, channels, 3, stride, 2, False),\n",
    "            DilConv(channels, channels, 5, stride, 2, False)\n",
    "        ]\n",
    "        self.op_names = ('maxpool', 'avgpool', 'skipconnect', 'sepconv3x3',\n",
    "                         'sepconv5x5', 'dilconv3x3', 'dilconv5x5')\n",
    "        \n",
    "    def __call__(self, x, is_training):\n",
    "        \"\"\"x: (bs, w, h, c)\"\"\"\n",
    "        alpha = hk.get_parameter(\"lc_alpha\", shape=(len(self.ops),), init=hk.initializers.RandomNormal(1e-3))\n",
    "        res = jnp.stack([op(x, is_training) for op in self.ops], axis=0)  # (# prev, bs, w, h, c)\n",
    "        # res = hk.fori_loop(0, len(self.ops), lambda i, h: self._operation(i, h, is_training),\n",
    "        #                    x[None].repeat(len(self.ops), axis=0))\n",
    "        weights = jax.nn.softmax(alpha, axis=-1).reshape(-1, 1, 1, 1, 1)\n",
    "        return (res * weights).sum(0)\n",
    "\n",
    "    # def _operation(self, i, x_stack, is_training):\n",
    "    #     \"\"\"x from (prev, bs, *)\"\"\"\n",
    "    #     # out = self.ops[self._id_to_op[i]](x_stack[i], is_training)  # (bs, *)\n",
    "    #     out = hk.switch(i, self.ops, x_stack[i], is_training)\n",
    "    #     return jnp.concatenate([x_stack[:i], out[None], x_stack[i + 1:]], axis=0)\n",
    "        \n",
    "\n",
    "class InputChoice(hk.Module):\n",
    "    def __init__(self, n_cand: int, n_chosen: int, label='none'):\n",
    "        super().__init__()\n",
    "        self.n_chosen = n_chosen\n",
    "        self.label = label\n",
    "        self.n_cand = n_cand\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        alpha = hk.get_parameter(\"ic_alpha\", shape=(self.n_cand,), dtype=jnp.float32,\n",
    "                                 init=hk.initializers.RandomNormal(1e-3))\n",
    "        inputs = jnp.stack(inputs, axis=0)  # (#cand, bs, w, h, c)\n",
    "        weights = jax.nn.softmax(alpha, axis=-1).reshape(-1, 1, 1, 1, 1)\n",
    "        return (inputs * weights).sum(0)\n",
    "\n",
    "\n",
    "class Node(hk.Module):\n",
    "    def __init__(self, node_id, num_prev_nodes, channels, num_downsample_connect, drop_path_prob):\n",
    "        super().__init__()\n",
    "        choice_keys = []\n",
    "        self.edges = []\n",
    "        for i in range(num_prev_nodes):\n",
    "            stride = 2 if i < num_downsample_connect else 1\n",
    "            choice_keys.append(f'{node_id}_p{i}')\n",
    "            self.edges.append(LayerChoice(channels, stride, choice_keys[-1]))\n",
    "        self.drop_path = DropPath(drop_path_prob)\n",
    "        self.input_switch = InputChoice(n_cand=len(choice_keys), n_chosen=2, label=f'{node_id}_switch')\n",
    "\n",
    "    def __call__(self, prev_nodes, is_training):\n",
    "        \"\"\"Prev nodes: List [bs, w, h, c]\"\"\"\n",
    "        out = [self.drop_path(edge(node, is_training), is_training) \\\n",
    "               for edge, node in zip(self.edges, prev_nodes)]\n",
    "        return self.input_switch(out)\n",
    "\n",
    "\n",
    "### TEST\n",
    "def test_layer_choice():\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    x = jnp.ones((3, 32, 32, 16))\n",
    "    lc = hk.transform_with_state(lambda x, is_t: LayerChoice(16, 1)(x, is_t))\n",
    "    params, state = lc.init(rng, x, True)\n",
    "    # print(params.keys())\n",
    "    out = lc.apply(params, state, rng, x, True)[0]  # out, state_new = apply \n",
    "    assert out.shape == x.shape\n",
    "\n",
    "\n",
    "def test_inp_choice():\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    x = jnp.ones((3, 32, 32, 16))\n",
    "    ic = hk.transform_with_state(lambda y: InputChoice(3, 2)(y))\n",
    "    params, state = ic.init(rng, [x, x, x])\n",
    "    out = ic.apply(params, state, rng, [x, x, x])[0]  # out, state_new = apply \n",
    "    assert out.shape == x.shape\n",
    "\n",
    "\n",
    "def test_node():\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    x = jnp.ones((3, 32, 32, 16))\n",
    "    node = hk.transform_with_state(lambda x, t: Node(3, 3, 16, 0, 0.1)(x, t))\n",
    "    params, state = node.init(rng, [x, x, x], True)\n",
    "    out = node.apply(params, state, rng, [x, x, x], True)[0]\n",
    "    assert out.shape == x.shape, f'{out.shape}'\n",
    "\n",
    "\n",
    "test_layer_choice()\n",
    "test_inp_choice()\n",
    "test_node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46e93e52-c41e-48fc-8939-3618a902f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cell(hk.Module):\n",
    "    def __init__(self, n_nodes, channels_pp, channels_p, channels, reduction_p, reduction, drop_path_prob):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        self.n_nodes = n_nodes\n",
    "\n",
    "        if reduction_p:\n",
    "            self.preproc0 = FactorizedReduce(channels, affine=False)\n",
    "        else:\n",
    "            self.preproc0 = StdConv(channels, 1, 1, affine=False)\n",
    "        self.preproc1 = StdConv(channels, 1, 1, affine=False)\n",
    "\n",
    "        # generate dag\n",
    "        self.mutable_ops = []\n",
    "        for depth in range(2, self.n_nodes + 2):\n",
    "            self.mutable_ops.append(Node(f\"{'reduce' if reduction else 'normal'}_n{depth}\",\n",
    "                                         depth, channels, 2 if reduction else 0, drop_path_prob))\n",
    "\n",
    "    def __call__(self, s0, s1, is_training):\n",
    "        inputs = [self.preproc0(s0, is_training), self.preproc1(s1, is_training)]\n",
    "        for node in self.mutable_ops:\n",
    "            out = node(inputs, is_training)\n",
    "            inputs.append(out)\n",
    "        out = jnp.concatenate(inputs[2:], axis=-1)  # (bs, w, h, c * nodes)\n",
    "        return out\n",
    "\n",
    "\n",
    "def test_cell():\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    x = jnp.ones((3, 32, 32, 16))\n",
    "    cell = hk.transform_with_state(lambda x, y, t: Cell(4, 3*16, 3 * 16, 16, False, False, 0.1)(x, y, t))\n",
    "    params, state = cell.init(rng, x, x, True)\n",
    "    out = cell.apply(params, state, rng, x, x, True)[0]\n",
    "    assert out.shape == (3, 32, 32, 64), f'{out.shape}'\n",
    "\n",
    "\n",
    "test_cell()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a82f3-1be0-4e75-8454-ed7e31c24645",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e196cb62-c5f3-4d98-9588-f5295bd408c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(hk.Module):\n",
    "    def __init__(self, channels, n_classes, n_layers, n_nodes=4,\n",
    "                stem_multiplier=3, drop_path_prob=0.0):\n",
    "        # TODO: add aux if necessary\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.n_classes = n_classes\n",
    "        self.n_layers = n_layers\n",
    "        c_cur = stem_multiplier * self.channels\n",
    "        self.stem = hk.Conv2D(c_cur, 3, stride=1, padding='SAME', with_bias=False)\n",
    "        self.stem_bn = hk.BatchNorm(True, True, 0.9)\n",
    "\n",
    "        # for the first cell, stem is used for both s0 and s1\n",
    "        # [!] channels_pp and channels_p is output channel size, but c_cur is input channel size.\n",
    "        channels_pp, channels_p, c_cur = c_cur, c_cur, channels\n",
    "\n",
    "        self.cells = []\n",
    "        reduction_p, reduction = False, False\n",
    "        for i in range(n_layers):\n",
    "            reduction_p, reduction = reduction, False\n",
    "            # Reduce featuremap size and double channels in 1/3 and 2/3 layer.\n",
    "            if i in [n_layers // 3, 2 * n_layers // 3]:\n",
    "                c_cur *= 2\n",
    "                reduction = True\n",
    "            cell = Cell(n_nodes, channels_pp, channels_p, c_cur, reduction_p, reduction, drop_path_prob)\n",
    "            self.cells.append(cell)\n",
    "            c_cur_out = c_cur * n_nodes\n",
    "            channels_pp, channels_p = channels_p, c_cur_out\n",
    "            \n",
    "        self.linear = hk.Linear(n_classes)\n",
    "\n",
    "\n",
    "    def __call__(self, x, is_training):\n",
    "        s0 = s1 = self.stem_bn(self.stem(x), is_training)\n",
    "        for i, cell in enumerate(self.cells):\n",
    "            # cell(s0, s1)\n",
    "            s0, s1 = s1, cell(s0, s1, is_training)\n",
    "\n",
    "        out = s1.mean(axis=[-1, -2])  # global adaptive pooling\n",
    "        out = out.reshape(out.shape[0], -1)  # flatten\n",
    "        logits = self.linear(out)\n",
    "        return logits\n",
    "\n",
    "def test_cnn():\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    x = jnp.ones((3, 32, 32, 3))\n",
    "    cnn = hk.transform_with_state(lambda x, t: CNN(16, 10, 1)(x, t))\n",
    "    params, state = cnn.init(rng, x, True)\n",
    "    out = cnn.apply(params, state, rng, x, True)[0]\n",
    "    assert out.shape == (3, 10)\n",
    "\n",
    "\n",
    "test_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f2d48ee-6be3-4629-a86f-d427a751b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "x = jnp.ones((3, 32, 32, 3))\n",
    "cnn = hk.transform_with_state(lambda x, t: CNN(16, 10, 1)(x, t))\n",
    "params, state = cnn.init(rng, x, True)\n",
    "out = cnn.apply(params, state, None, x, True)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42403fee-ce4f-46d1-b60f-9511b0d8c4a4",
   "metadata": {},
   "source": [
    "### 1-layer network => do not share parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e621c358-fb3c-4d0f-8fa0-9affdf18ff48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([{'ic_alpha': Array([ 0.00158999, -0.0003824 ], dtype=float32)}, {'lc_alpha': Array([-0.00039745,  0.00262213, -0.00046866,  0.00169459,  0.00027594,\n",
       "        0.00163345, -0.00016987], dtype=float32)}, {'lc_alpha': Array([-0.00011725, -0.00021481, -0.0002148 , -0.00189549, -0.00110976,\n",
       "        0.00219348,  0.00024185], dtype=float32)}, {'ic_alpha': Array([ 0.00077681, -0.00016473, -0.0002152 ], dtype=float32)}, {'lc_alpha': Array([-0.00129291, -0.00086983,  0.00018381,  0.00123449,  0.00072178,\n",
       "        0.0011726 , -0.00187813], dtype=float32)}, {'lc_alpha': Array([-0.00066265,  0.00143617, -0.0008633 , -0.001704  , -0.00163068,\n",
       "        0.00021376,  0.00086577], dtype=float32)}, {'lc_alpha': Array([ 0.00083637, -0.00041961, -0.0019002 , -0.00166142, -0.00178719,\n",
       "       -0.00059523,  0.00122192], dtype=float32)}, {'ic_alpha': Array([-0.00189897, -0.00062385, -0.00120795,  0.00092347], dtype=float32)}, {'lc_alpha': Array([ 0.00067808, -0.00017294,  0.00027677,  0.0011895 ,  0.00136863,\n",
       "       -0.00017513, -0.00074789], dtype=float32)}, {'lc_alpha': Array([ 0.00033506, -0.00177405, -0.00067666, -0.0013877 ,  0.0005867 ,\n",
       "        0.00071412,  0.00069788], dtype=float32)}, {'lc_alpha': Array([ 0.00086902, -0.00018407,  0.00016538,  0.00127223,  0.0007007 ,\n",
       "        0.00056835,  0.00192441], dtype=float32)}, {'lc_alpha': Array([-5.6094788e-05,  2.9816531e-04,  1.6715542e-04, -4.3988967e-04,\n",
       "       -1.3141103e-04,  1.5671873e-04,  1.0789896e-03], dtype=float32)}, {'ic_alpha': Array([ 7.6457771e-04, -5.4779182e-05,  2.9192970e-04,  4.0215300e-04,\n",
       "        1.6530056e-03], dtype=float32)}, {'lc_alpha': Array([-0.00052262, -0.00021769, -0.00079446, -0.00116417, -0.00113388,\n",
       "        0.00050822,  0.00071892], dtype=float32)}, {'lc_alpha': Array([-4.3458171e-04, -7.8727450e-04,  9.5970667e-05,  4.8888265e-04,\n",
       "        3.6048013e-04, -3.1567362e-04,  8.4898830e-04], dtype=float32)}, {'lc_alpha': Array([-8.9276483e-04,  1.2304831e-03,  7.7024197e-05, -4.7029171e-04,\n",
       "        3.4982042e-04, -6.2994304e-04,  5.5922038e-04], dtype=float32)}, {'lc_alpha': Array([ 3.4475679e-04,  1.9359753e-04,  8.0324928e-05, -8.7835116e-04,\n",
       "       -7.1849812e-05,  6.3893765e-07, -1.8287663e-03], dtype=float32)}, {'lc_alpha': Array([ 3.1528060e-04, -4.1668358e-05,  9.9074282e-04, -4.4925584e-04,\n",
       "        9.0255495e-04,  2.6210991e-03, -5.4848380e-04], dtype=float32)}])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_params, h_params = hk.data_structures.partition(lambda m, n, p: 'alpha' not in n, params)\n",
    "h_params.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff31d389-0350-47fe-85a6-381fc246f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import graphviz\n",
    "# dot = hk.experimental.to_dot(cnn.apply)(params, state, rng, x, True)\n",
    "# src = graphviz.Source(dot)\n",
    "# src.render('cnn', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50f53b65-3fa1-435d-aec7-07fb492abc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, i in enumerate(hk.experimental.eval_summary(lambda x: cnn.apply(params, state, rng, x, True))(x)):\n",
    "#     print(\"mod := {:14} | in := {} out := {}\".format(i.module_details.module.module_name,\n",
    "#                                                      i.args_spec[0], i.output_spec))\n",
    "#     if idx >= 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc0d245-29cc-4d6c-8588-29b24d83a7c8",
   "metadata": {},
   "source": [
    "## Bilevel state and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e72d767c-ebd2-4f0e-ba14-d7c695ea41ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    accuracy: metrics.Accuracy\n",
    "    loss: metrics.Average.from_output('loss')\n",
    "\n",
    "\n",
    "class NasTrainState(struct.PyTreeNode):\n",
    "    rng: jax.Array\n",
    "    metrics: Metrics\n",
    "    step: int\n",
    "    apply_fn: Callable = struct.field(pytree_node=False)\n",
    "    w_params: core.FrozenDict[str, Any] = struct.field(pytree_node=True)\n",
    "    h_params: core.FrozenDict[str, Any] = struct.field(pytree_node=True)\n",
    "    bn_state: core.FrozenDict[str, Any] = struct.field(pytree_node=True)\n",
    "    inner_opt: optax.GradientTransformation = struct.field(pytree_node=False)\n",
    "    inner_opt_state: optax.OptState = struct.field(pytree_node=True)\n",
    "    outer_opt: optax.GradientTransformation = struct.field(pytree_node=False)\n",
    "    outer_opt_state: optax.OptState = struct.field(pytree_node=True)\n",
    "    lr: float  # inner lr\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, *, apply_fn, w_params, h_params, bn_state, inner_opt, outer_opt, **kwargs):\n",
    "        inner_opt_state = inner_opt.init(w_params)\n",
    "        outer_opt_state = outer_opt.init(h_params)\n",
    "        return cls(\n",
    "            step=0,\n",
    "            apply_fn=apply_fn,\n",
    "            w_params=w_params,\n",
    "            h_params=h_params,\n",
    "            bn_state=bn_state,\n",
    "            inner_opt=inner_opt,\n",
    "            outer_opt=outer_opt,\n",
    "            inner_opt_state=inner_opt_state,\n",
    "            outer_opt_state=outer_opt_state,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def apply_w_gradients(self, *, w_grads, **kwargs):\n",
    "        updates, new_inn_state = self.inner_opt.update(w_grads, self.inner_opt_state, self.w_params)\n",
    "        new_params = optax.apply_updates(self.w_params, updates)\n",
    "        rng, _ = jax.random.split(self.rng)\n",
    "        return self.replace(\n",
    "            step=self.step + 1,\n",
    "            w_params=new_params,\n",
    "            inner_opt_state=new_inn_state,\n",
    "            rng=rng,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def apply_h_gradients(self, *, h_grads, **kwargs):\n",
    "        updates, new_out_state = self.outer_opt.update(h_grads, self.outer_opt_state, self.h_params)\n",
    "        new_params = optax.apply_updates(self.h_params, updates)\n",
    "        return self.replace(\n",
    "            step=self.step + 1,\n",
    "            h_params=new_params,\n",
    "            outer_opt_state=new_out_state,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "\n",
    "def create_nas_train_state(module, rng, learning_rate=0.025, momentum=0.9, w_decay=3e-4,\n",
    "                               alpha_lr=1e-4, alpha_decay=1e-3):\n",
    "    \"\"\"Creates an initial `TrainState`.\"\"\"\n",
    "    params, bn_state = module.init(rng, jnp.ones([1, 32, 32, 3]), True)\n",
    "    w_params, h_params = hk.data_structures.partition(lambda m, n, p: 'alpha' not in n, params)\n",
    "    tx_inner = optax.chain(optax.add_decayed_weights(w_decay),\n",
    "                           optax.sgd(learning_rate, momentum=momentum))\n",
    "    tx_outer = optax.chain(optax.add_decayed_weights(alpha_decay), optax.adam(alpha_lr, b1=0.5, b2=0.999))\n",
    "    return NasTrainState.create(\n",
    "      apply_fn=module.apply, w_params=w_params, h_params=h_params, bn_state=bn_state, inner_opt=tx_inner, outer_opt=tx_outer,\n",
    "      metrics=Metrics.empty(), lr=learning_rate, rng=rng)\n",
    "\n",
    "\n",
    "\n",
    "conv_net = hk.transform_with_state(lambda x, t: CNN(16, 10, 1, drop_path_prob=0.1)(x, t))\n",
    "rng = jax.random.PRNGKey(42)\n",
    "state = create_nas_train_state(conv_net, jax.random.PRNGKey(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d71df44-ff2c-4c7b-9194-82c5ab704cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def proc(st):\n",
    "#     st = st.replace(bn_state=jax.tree_util.tree_map(jnp.zeros_like, st.bn_state))\n",
    "#     print(st.bn_state['cnn/~/batch_norm/~/mean_ema'])\n",
    "\n",
    "# print(state.bn_state['cnn/~/batch_norm/~/mean_ema'])\n",
    "# proc(state)\n",
    "# print(state.bn_state['cnn/~/batch_norm/~/mean_ema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc06c1da-05de-49f2-b6e6-215e7894a6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(x.size for x in jax.tree_util.tree_leaves(state.w_params)) // 1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a096b957-52f7-4cf6-a7ac-6028a51dd44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def foo(x):\n",
    "#     return x ** 2, 123\n",
    "\n",
    "# jax.value_and_grad(foo, has_aux=True)(6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d7917b-99ba-44d5-b95f-e07076f8fc29",
   "metadata": {},
   "source": [
    "### Dataset: Cifar10, only train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d14191e-df38-48b5-83b0-1894b3c51ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]\n",
    "CIFAR_STD = [0.24703233, 0.24348505, 0.26158768]\n",
    "\n",
    "class ToNumpy:\n",
    "  def __call__(self, pic):\n",
    "    return np.asarray(pic.permute(1, 2, 0), dtype=np.float32)\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "    ToNumpy(),\n",
    "  ])\n",
    "\n",
    "train_data = torchvision.datasets.CIFAR10(root='.', train=True, download=True, transform=train_transform)\n",
    "\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(0.5 * num_train))\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#   train_data, batch_size=64,\n",
    "#   sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),\n",
    "#   pin_memory=True, num_workers=2)\n",
    "\n",
    "train_loader = jdl.DataLoader(torch.utils.data.Subset(train_data, range(split)), 'pytorch',\n",
    "                              batch_size=64, shuffle=True, drop_last=True)\n",
    "\n",
    "# valid_loader = torch.utils.data.DataLoader(\n",
    "#   train_data, batch_size=64,\n",
    "#   sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:num_train]),\n",
    "#   pin_memory=True, num_workers=2)\n",
    "\n",
    "valid_loader = jdl.DataLoader(torch.utils.data.Subset(train_data, range(split, len(train_data))), 'pytorch',\n",
    "                             batch_size=64, shuffle=True, dtop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b1baea2-b726-4fef-92df-9239f989d568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 32, 32, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "42654006-d9f2-4b0a-8df3-2f2db0b8ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit\n",
    "def loss_fn(w_params, h_params, state, batch, is_training=True):\n",
    "    params = hk.data_structures.merge(w_params, h_params)\n",
    "    logits, bn_state = state.apply_fn(params, state.bn_state, state.rng,\n",
    "                                          batch['image'], is_training)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=logits, labels=batch['label']).mean()\n",
    "    return loss, state.replace(bn_state=bn_state)\n",
    "\n",
    "# @jax.jit\n",
    "def compute_metrics(*, state, batch):\n",
    "    params = hk.data_structures.merge(state.w_params, state.h_params)\n",
    "    logits, _ = state.apply_fn(params, state.bn_state, None, batch['image'], False)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch['label']).mean()\n",
    "    metric_updates = state.metrics.single_from_model_output(logits=logits, labels=batch['label'], loss=loss)\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "def inner_step(state: NasTrainState, batch):    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, argnums=0, has_aux=True)\n",
    "    (_, state), grads = grad_fn(state.w_params, state.h_params, state, batch)\n",
    "    state = state.apply_w_gradients(w_grads=grads)\n",
    "    return state\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "def B_jvp(w_params, h_params, batch, state, v, eps=1e-7):\n",
    "    \"\"\"d^2 L1 / dl dw v\"\"\"\n",
    "    w_plus = jax.tree_util.tree_map(lambda x, y: x + eps * y, w_params, v)\n",
    "    w_minus = jax.tree_util.tree_map(lambda x, y: x - eps * y, w_params, v)\n",
    "    dl_dlam = jax.grad(loss_fn, argnums=1, has_aux=True)\n",
    "    # TODO: think of updating bn state\n",
    "    g_plus = dl_dlam(w_plus, h_params, state, batch)[0]\n",
    "    g_minus = dl_dlam(w_minus, h_params, state, batch)[0]\n",
    "    return jax.tree_util.tree_map(lambda x, y: -state.lr * (x - y) / (2 * eps), g_plus, g_minus)\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "def A_jvp(w_params, batch, state, v, eps=1e-7):\n",
    "    w_plus = jax.tree_util.tree_map(lambda x, y: x + eps * y, w_params, v)\n",
    "    w_minus = jax.tree_util.tree_map(lambda x, y: x - eps * y, w_params, v)\n",
    "    dl_dw = jax.grad(loss_fn, argnums=0, has_aux=True)\n",
    "    # TODO: think of updating bn state\n",
    "    g_plus = dl_dw(w_plus, state.h_params, state, batch)[0]\n",
    "    g_minus = dl_dw(w_minus, state.h_params, state, batch)[0]\n",
    "    hvp = jax.tree_util.tree_map(lambda x, y: (x - y) / (2 * eps), g_plus, g_minus)\n",
    "    return jax.tree_util.tree_map(lambda x, y: x - state.lr * y, v, hvp)\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "def fo_grad(state, val_batch):\n",
    "    return jax.grad(loss_fn, argnums=1, has_aux=True)(state.w_params, state.h_params, state, val_batch)[0]\n",
    "\n",
    "\n",
    "def drmad_grad(state, batches, val_batch):\n",
    "    \"\"\"T = len(batches)\"\"\"\n",
    "    T = len(batches)\n",
    "    g_so = jax.tree_util.tree_map(jnp.zeros_like, state.h_params)\n",
    "    w_0 = state.w_params\n",
    "    for step, batch in enumerate(batches):\n",
    "        state = inner_step(state, batch)\n",
    "    w_T = state.w_params\n",
    "    alpha = jax.grad(loss_fn, argnums=0, has_aux=True)(state.w_params, state.h_params, state, val_batch)[0]\n",
    "    for step, batch in enumerate(batches[::-1]):\n",
    "        t = T - step\n",
    "        w_tm1 = jax.tree_util.tree_map(lambda x, y: (1 - (t - 1) / T) * x + (t - 1) / T * y, w_0, w_T)\n",
    "        g_so = jax.tree_util.tree_map(lambda x, y: x + y, B_jvp(w_tm1, state.h_params, batch, state, alpha), g_so)\n",
    "        # update alpha\n",
    "        if step != len(batches) - 1:\n",
    "            alpha = A_jvp(w_tm1, batch, state, alpha)\n",
    "    return state, g_so\n",
    "\n",
    "\n",
    "def proposed_so_grad(state, batches, val_batch, gamma):\n",
    "    \"\"\"T = len(batches)\"\"\"\n",
    "    g_so = jax.tree_util.tree_map(jnp.zeros_like, state.h_params)\n",
    "    T = len(batches)\n",
    "    for step, batch in enumerate(batches):\n",
    "        new_state = inner_step(state, batch)\n",
    "        curr_alpha = jax.grad(loss_fn, argnums=0, has_aux=True)(new_state.w_params, state.h_params, state, val_batch)[0]\n",
    "        g_so = jax.tree_util.tree_map(lambda x, y: x * gamma ** (T - 1 - step) + y,\n",
    "                                      B_jvp(state.w_params, state.h_params, batch,\n",
    "                                            state, curr_alpha),\n",
    "                                     g_so)\n",
    "        state = new_state\n",
    "    return state, g_so\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3d32aa1e-f00d-4f10-b287-680303be050d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███                                                                                                  | 6/200 [00:45<24:40,  7.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m     g_so \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mtree_map(jnp\u001b[38;5;241m.\u001b[39mzeros_like, state\u001b[38;5;241m.\u001b[39mh_params)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDrMAD\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 40\u001b[0m     state, g_so \u001b[38;5;241m=\u001b[39m \u001b[43mdrmad_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m method)\n",
      "Cell \u001b[0;32mIn[87], line 66\u001b[0m, in \u001b[0;36mdrmad_grad\u001b[0;34m(state, batches, val_batch)\u001b[0m\n\u001b[1;32m     64\u001b[0m     state \u001b[38;5;241m=\u001b[39m inner_step(state, batch)\n\u001b[1;32m     65\u001b[0m w_T \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mw_params\n\u001b[0;32m---> 66\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_batch\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batches[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m     68\u001b[0m     t \u001b[38;5;241m=\u001b[39m T \u001b[38;5;241m-\u001b[39m step\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/jax-metal/lib/python3.9/site-packages/jax/_src/api.py:647\u001b[0m, in \u001b[0;36mgrad.<locals>.grad_f_aux\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fun, docstr\u001b[38;5;241m=\u001b[39mdocstr, argnums\u001b[38;5;241m=\u001b[39margnums)\n\u001b[1;32m    645\u001b[0m \u001b[38;5;129m@api_boundary\u001b[39m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad_f_aux\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 647\u001b[0m   (_, aux), g \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_and_grad_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m g, aux\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/jax-metal/lib/python3.9/site-packages/jax/_src/api.py:723\u001b[0m, in \u001b[0;36mvalue_and_grad.<locals>.value_and_grad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m _check_scalar(ans)\n\u001b[1;32m    722\u001b[0m tree_map(partial(_check_output_dtype_grad, holomorphic), ans)\n\u001b[0;32m--> 723\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mvjp_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlax_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mans\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m g \u001b[38;5;241m=\u001b[39m g[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(argnums, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m g\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_aux:\n",
      "File \u001b[0;32m~/jax-metal/lib/python3.9/site-packages/jax/_src/tree_util.py:307\u001b[0m, in \u001b[0;36m_HashableCallableShim.__call__\u001b[0;34m(self, *args, **kw)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m--> 307\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jax-metal/lib/python3.9/site-packages/jax/_src/api.py:2098\u001b[0m, in \u001b[0;36m_vjp_pullback_wrapper\u001b[0;34m(name, cotangent_dtypes, cotangent_shapes, io_tree, fun, *py_args_)\u001b[0m\n\u001b[1;32m   2093\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mshape(arg) \u001b[38;5;241m!=\u001b[39m ct_shape:\n\u001b[1;32m   2094\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2095\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of cotangent input to vjp pullback function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mshape(arg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2096\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust be the same as the shape of corresponding primal input \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2097\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mct_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2098\u001b[0m ans \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(out_tree, ans)\n",
      "File \u001b[0;32m~/jax-metal/lib/python3.9/site-packages/jax/_src/tree_util.py:307\u001b[0m, in \u001b[0;36m_HashableCallableShim.__call__\u001b[0;34m(self, *args, **kw)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m--> 307\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jax-metal/lib/python3.9/site-packages/jax/_src/interpreters/ad.py:146\u001b[0m, in \u001b[0;36mvjp.<locals>.unbound_vjp\u001b[0;34m(pvals, jaxpr, consts, *cts)\u001b[0m\n\u001b[1;32m    144\u001b[0m cts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(ct \u001b[38;5;28;01mfor\u001b[39;00m ct, pval \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(cts, pvals) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pval\u001b[38;5;241m.\u001b[39mis_known())\n\u001b[1;32m    145\u001b[0m dummy_args \u001b[38;5;241m=\u001b[39m [UndefinedPrimal(v\u001b[38;5;241m.\u001b[39maval) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m jaxpr\u001b[38;5;241m.\u001b[39minvars]\n\u001b[0;32m--> 146\u001b[0m arg_cts \u001b[38;5;241m=\u001b[39m \u001b[43mbackward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(instantiate_zeros, arg_cts)\n",
      "File \u001b[0;32m~/jax-metal/lib/python3.9/site-packages/jax/_src/interpreters/ad.py:253\u001b[0m, in \u001b[0;36mbackward_pass\u001b[0;34m(jaxpr, reduce_axes, transform_stack, consts, primals_in, cotangents_in)\u001b[0m\n\u001b[1;32m    250\u001b[0m   cts_out \u001b[38;5;241m=\u001b[39m reducing_transposes[eqn\u001b[38;5;241m.\u001b[39mprimitive](\n\u001b[1;32m    251\u001b[0m       reduce_axes, cts_in, \u001b[38;5;241m*\u001b[39minvals, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39meqn\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m   cts_out \u001b[38;5;241m=\u001b[39m \u001b[43mget_primitive_transpose\u001b[49m\u001b[43m(\u001b[49m\u001b[43meqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprimitive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcts_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m cts_out \u001b[38;5;241m=\u001b[39m [Zero(v\u001b[38;5;241m.\u001b[39maval) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m eqn\u001b[38;5;241m.\u001b[39minvars] \u001b[38;5;28;01mif\u001b[39;00m cts_out \u001b[38;5;129;01mis\u001b[39;00m Zero \u001b[38;5;28;01melse\u001b[39;00m cts_out\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# FIXME: Some invars correspond to primals!\u001b[39;00m\n",
      "File \u001b[0;32m~/jax-metal/lib/python3.9/site-packages/jax/_src/interpreters/ad.py:570\u001b[0m, in \u001b[0;36mbilinear_transpose\u001b[0;34m(lhs_rule, rhs_rule, cotangent, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m Zero \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m Zero \u001b[38;5;28;01melse\u001b[39;00m (out, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 570\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mrhs_rule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcotangent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m Zero \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m Zero \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, out)\n",
      "File \u001b[0;32m~/jax-metal/lib/python3.9/site-packages/jax/_src/lax/convolution.py:519\u001b[0m, in \u001b[0;36m_conv_general_dilated_transpose_rhs\u001b[0;34m(g, lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation, dimension_numbers, feature_group_count, batch_group_count, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m    514\u001b[0m trans_dimension_numbers \u001b[38;5;241m=\u001b[39m ConvDimensionNumbers(lhs_trans, out_trans, rhs_trans)\n\u001b[1;32m    515\u001b[0m padding \u001b[38;5;241m=\u001b[39m _conv_general_vjp_rhs_padding(\n\u001b[1;32m    516\u001b[0m     np\u001b[38;5;241m.\u001b[39mtake(lhs_shape, lhs_sdims), np\u001b[38;5;241m.\u001b[39mtake(rhs_shape, rhs_sdims),\n\u001b[1;32m    517\u001b[0m     window_strides, np\u001b[38;5;241m.\u001b[39mtake(g\u001b[38;5;241m.\u001b[39mshape, out_sdims), padding, lhs_dilation,\n\u001b[1;32m    518\u001b[0m     rhs_dilation)\n\u001b[0;32m--> 519\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconv_general_dilated\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_strides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrhs_dilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlhs_dilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlhs_dilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs_dilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_strides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimension_numbers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrans_dimension_numbers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_group_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_group_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_group_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_group_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jax-metal/lib/python3.9/site-packages/jax/_src/lax/convolution.py:161\u001b[0m, in \u001b[0;36mconv_general_dilated\u001b[0;34m(lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation, dimension_numbers, feature_group_count, batch_group_count, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpadding argument to conv_general_dilated should be a string or a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence of (low, high) pairs, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpadding\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    158\u001b[0m preferred_element_type \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m preferred_element_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(np\u001b[38;5;241m.\u001b[39mdtype(preferred_element_type)))\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconv_general_dilated_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_strides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwindow_strides\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlhs_dilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlhs_dilation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs_dilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrhs_dilation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimension_numbers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdnums\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_group_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_group_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_group_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_group_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanonicalize_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jax-metal/lib/python3.9/site-packages/jax/_src/core.py:380\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    378\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_enable_checks \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    379\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 380\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jax-metal/lib/python3.9/site-packages/jax/_src/core.py:383\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 383\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/jax-metal/lib/python3.9/site-packages/jax/_src/core.py:790\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 790\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jax-metal/lib/python3.9/site-packages/jax/_src/dispatch.py:144\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    140\u001b[0m   msg \u001b[38;5;241m=\u001b[39m pjit\u001b[38;5;241m.\u001b[39m_device_assignment_mismatch_error(\n\u001b[1;32m    141\u001b[0m       prim\u001b[38;5;241m.\u001b[39mname, fails, args, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjit\u001b[39m\u001b[38;5;124m'\u001b[39m, arg_names)\n\u001b[1;32m    142\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jax-metal/lib/python3.9/site-packages/jax/_src/dispatch.py:227\u001b[0m, in \u001b[0;36mxla_primitive_callable.<locals>.<lambda>\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    223\u001b[0m compiled \u001b[38;5;241m=\u001b[39m _xla_callable_uncached(\n\u001b[1;32m    224\u001b[0m     lu\u001b[38;5;241m.\u001b[39mwrap_init(prim_fun), prim\u001b[38;5;241m.\u001b[39mname, donated_invars, \u001b[38;5;28;01mFalse\u001b[39;00m, in_avals,\n\u001b[1;32m    225\u001b[0m     orig_in_shardings)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prim\u001b[38;5;241m.\u001b[39mmultiple_results:\n\u001b[0;32m--> 227\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: \u001b[43mcompiled\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m compiled\n",
      "File \u001b[0;32m~/jax-metal/lib/python3.9/site-packages/jax/_src/profiler.py:314\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/jax-metal/lib/python3.9/site-packages/jax/_src/interpreters/pxla.py:1355\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_token_bufs(\n\u001b[1;32m   1351\u001b[0m       results\u001b[38;5;241m.\u001b[39mdisassemble_prefix_into_single_device_arrays(\n\u001b[1;32m   1352\u001b[0m           \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mordered_effects)),\n\u001b[1;32m   1353\u001b[0m       results\u001b[38;5;241m.\u001b[39mconsume_token())\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1355\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxla_executable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mneeds_check_special():\n\u001b[1;32m   1357\u001b[0m   out_arrays \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mdisassemble_into_single_device_arrays()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seeds=(0,)\n",
    "\n",
    "\n",
    "# TODO: deal with new bn state on validation. Should we update it? In the original DARTS we do\n",
    "T = 1\n",
    "method = 'proposed'; gamma=0.001\n",
    "method = 'DrMAD'\n",
    "# method = 'fo'\n",
    "\n",
    "metrics_history = {seed: {'train_loss': [],\n",
    "                   'train_accuracy': [],\n",
    "                   'test_loss': [],\n",
    "                   'test_accuracy': []} for seed in seeds}\n",
    "\n",
    "# TODO: swithc to bigger setup\n",
    "conv_net = hk.transform_with_state(lambda x, t: CNN(4, 10, 1, drop_path_prob=0.1)(x, t))  # 16 channels\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f'Seed: {seed}')\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    state = create_nas_train_state(conv_net, jax.random.PRNGKey(seed))\n",
    "\n",
    "    for outer_step in tqdm(range(200)):\n",
    "        \n",
    "        x_val, y_val = next(iter(valid_loader))\n",
    "        val_batch = {'image': jnp.asarray(x_val), 'label': jnp.asarray(y_val)}\n",
    "        batches = []\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            if i >= T:\n",
    "                break\n",
    "            batches.append({'image': jnp.asarray(x), 'label': jnp.asarray(y)})\n",
    "        if method == 'proposed':\n",
    "            state, g_so = proposed_so_grad(state, batches, val_batch, gamma)\n",
    "        elif method == 'fo':\n",
    "            for batch in batches:\n",
    "                state = inner_step(state, batch)\n",
    "            g_so = jax.tree_util.tree_map(jnp.zeros_like, state.h_params)\n",
    "        elif method == 'DrMAD':\n",
    "            state, g_so = drmad_grad(state, batches, val_batch)\n",
    "        else:\n",
    "            raise ValueError('Unknown ' + method)\n",
    "\n",
    "        \n",
    "        g_fo = fo_grad(state, val_batch)\n",
    "        state = state.apply_h_gradients(h_grads=jax.tree_util.tree_map(lambda x, y: x + y, g_fo, g_so))\n",
    "\n",
    "        # eval\n",
    "        if outer_step % 20 == 0 and outer_step > 0:\n",
    "            for x, y in valid_loader:\n",
    "                val_batch = {'image': jnp.asarray(x), 'label': jnp.asarray(y)}\n",
    "                state = compute_metrics(state=state, batch=val_batch)\n",
    "            for metric,value in state.metrics.compute().items():\n",
    "                metrics_history[seed][f'test_{metric}'].append(value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "935c62ca-ceb5-4923-8ffd-7ed7737c7c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11941106])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc_arr = np.stack([metrics_history[s]['test_accuracy'] for s in seeds], axis=0)\n",
    "plt.plot(acc_arr.mean(0))\n",
    "plt.fill_between(range(acc_arr.shape[1]), acc_arr.mean(0) - acc_arr.std(0), acc_arr.mean(0) + acc_arr.std(0), alpha=0.3)\n",
    "\n",
    "\n",
    "# np.save('ANIL_results/results_fo.npy', acc_arr)\n",
    "# np.save(f'ANIL_results/results_proposed_{gamma}.npy', acc_arr)\n",
    "# np.save(f'ANIL_results/results_DrMAD.npy', acc_arr)\n",
    "\n",
    "# ??? why no test loss???\n",
    "acc_arr.max(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b69ebb-814a-41e1-b931-8da019b42704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
